---
published : true
layout : post
categories : LLM, AI, LoRA
title : LoRA 논문 리뷰
tags : [LLM, AI, LoRA]
date-string : July 24, 2025
---

# LoRA
## 요약
- 학습된 모델의 가중치를 고정
- 훈련 가능한 분해 가중치를 트랜스포머 아키텍처의 각 레이어에 주입
    - 튜닝에 필요한 파라미터 수를 줄임


## LLM 튜닝
- LLM이 있다고 가정
    - 필요하면 파인튜닝
    - 이 때 모든 가중치를 업데이트
    - 이 경우 시간과 비용이 많이 소모됨

## LoRA 이전 방식

- 특정한 태스크의 파라미터들을 Pretrained Model에 주입
    - 추론 지연 발생
        - 모델 깊이를 연장시키거나, 모델의 사용 가능한 시퀀스 길이를 줄이기 때문.
    - 효율성과 모델 성능의 트레이드 오프 조정도 실패