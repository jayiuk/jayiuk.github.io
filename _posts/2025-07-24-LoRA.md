---
published : true
layout : post
categories : LLM, AI, LoRA
title : LoRA 논문 리뷰
tags : [LLM, AI, LoRA]
date-string : July 24, 2025
---

# LoRA
## 요약
- 학습된 모델의 가중치를 고정
- 훈련 가능한 분해 가중치를 트랜스포머 아키텍처의 각 레이어에 주입
    - 튜닝에 필요한 파라미터 수를 줄임


## LLM 튜닝
- LLM이 있다고 가정
    - 필요하면 파인튜닝
    - 이 때 모든 가중치를 업데이트
    - 이 경우 시간과 비용이 많이 소모됨

## LoRA 이전 방식

- 특정한 태스크의 파라미터들을 Pretrained Model에 주입
    - 추론 지연 발생
        - 모델 깊이를 연장시키거나, 모델의 사용 가능한 시퀀스 길이를 줄이기 때문.
    - 효율성과 모델 성능의 트레이드 오프 조정도 실패

## LoRA의 이론적 기반
- 과도하게 파라미터가 많은 모델이 실제로 낮은 고유 차원에 있다
    - 파라미터가 아무리 많아도 실제로 사용하는 파라미터 수는 적다
    - => 성능 향상에 영향을 주는 파라미터는 전체 파라미터의 일부
    - => 모델 adaptation하는 동안 가중치 변화도 일부만 될 것이다

## LoRA의 장점
- LoRA 모듈을 통해 다른 작업에 더 효과적으로 대응 가능
- 학습 효율
    - 랭크 분해 행렬만 최적화 시키기 때문
- 풀 파인튜닝에 비해 더 적은 추론 지연

## LoRA의 간략한 설명
- 트랜스포머 레이어 dmodel의 인풋과 아웃풋 사이즈를 불러옴
- self-attention 모듈에 있는 query/key/value/output 투영행렬 $$W_q, W_k, W_v, W_o$$사용
- $$W, W_{\theta}$$는 학습된 가중치 행렬
- $$\Delta W$$는 adaption동안 업데이트
    - $$\Delta W$$는 adaptation 동안의 변화량
        - gradient(손실함수의 기울기)들이 누적되면 변화량을 뜻함
- r = LoRA 모듈의 랭크
- 트랜스포머 구조를 그대로 참조


## 기존 파인튜닝
- 하나의 LLM이 있다고 가정
    - 각기 다른 특정 작업에선 성능이 좋진 않다
    - 이 문제를 해소하기 위해선 파인튜닝 필요
        - 이 경우 $$X = {(x_i, y_i)}_{i = 1, 2, ..., N}$$ 형식의 데이터셋 필요
    - 재학습 시 기존의 파라미터 만큼의 새로운 파라미터 학습
        -> 모델 저장 및 배포에 어려움이 생긴다
