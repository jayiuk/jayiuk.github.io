---
published : true
layout : post
categories : AI, RecommendSystem
title : Wide&Deep 논문 리뷰
tags : [AI, RecommendSystem]
date-string : September 07, 2025
---

# Wide & Deep

## Abstract
- 특성간 상호작용을 cross-product 형태로 기억하는 것은 효과적이고, 해석가능함
- 그러나 일반화에 있어선 더 많은 feature engineering 필요
- 딥러닝 신경망은 일반화에 더 효과적
    - feature engineering을 덜해도 딥러닝 신경망은 보지 못했던 특성 조합에 대해서도 더 일반화 가능
        - 희소한 특성을 저차원의 밀집 벡터로 표현하기 때문
- 하지만 이런 딥러닝 신경망은 유저-아이템 상호작용이 희소하고 고차원 랭크일 때 지나치게 일반화해 관련 없는 것들을 추천할 수 있다
    - 사용자-아이템 상호작용이 적으면 임베딩이 충분히 학습되지 못함
        - 유저-아이템 상호작용을 저차원 밀집 벡터로 임베딩
        - 이 과정에서 비슷한 아이템/유저를 가까운 벡터로 표현
    - 모델은 비슷해 보이는 다른 아이템으로 일반화 -> 정확도 하락

- Wide&Deep
    - linear model과 deep neural network를 함께 학습
    - 추천 시스템에서 memorization과 generalization 의 이점을 함께 활용

## Introduction
- 추천 시스템과 검색 랭킹 시스템에서 주요한 문제는 memorization, generalization
    - momorization : 아이템이나 특성의 같이 나타나는 빈도를 학습, 데이터에서 상관성을 얻는 것
    - generalization : 상관관계의 전이성을 바탕으로 과거에 없거나 매우 적은 새로운 특성 조합을 발견하는것
    - memorization은 주제에 알맞고 과거를 바탕으로 결과가 나오기 때문에 더 직관적
    - generalization은 추천된 아이템의 다양성을 높인다
- 산업에서 대규모 온라인 추천시스템과 랭킹시스템에서 로지스틱 회귀같은 일반화된 선형 모델이 주로 쓰인다
    - 간단, 해석 가능
- 모델들은 종종 원-핫 인코딩 된 이진 희소 특성들로 훈련됨
- Memorization은 희소 특성들을 cross-product로 변환한 것으로 효율적으로 습득 가능
    - AND(user_installed_app=netflix, impression_app=pandora)
    - 만약 유저가 넷플릭스를 설치했고, 이후에 판도라가 노출되면 1
    - 이는 특성 쌍의 동시 출현이 타겟 라벨과 어떻게 상관시키는지 설명한다.
- Generalization은 덜 세분화된 특성들로 더해질 수 있다.
    - 그러나 종종 feature engineering이 필요하다
- _cross-product의 하나의 제한점은 훈련 데이터에 없던 특성 쌍들을 일반화 하지 못한다는 것_

- factorization machines 혹은 deep neural network 같은 임베딩 기반 모델들은 더 적은 feature engineering으로 쿼리, 아이템 각각의 특징들을 저차원 밀집 임베딩 벡터로 학습해 이전의 보이지 않았던 query-item 특징 쌍을 일반화 할 수 있다
- 그러나 특이취향 유저나 적게 노출되는 맞춤 아이템 같이 쿼리-아이템 행렬이 희소하고 고랭크일 때 쿼리와 아이템에 대한 저차원 표현을 학습하기 힘들어진다
    - 이 경우 대부분의 쿼리-아이템 쌍들과 교류가 없어도 dense-embedding은 아무 이유 없이 모든 쿼리-아이템 쌍 예측으로 이끈다
    - 이는 과도한 일반화나 정확도를 낮춘다
- _그러나 cross-product(교차곱) 특성 변환을 하는 선형 모델은 이런 예외적인 규칙을 더 적은 파라미터로 기억 가능_

![wide&deep](https://github.com/user-attachments/assets/0d6f7856-8481-4964-8e4e-e939404e0b95)

## WIDE & DEEP LEARNING
### Wide Component
- wide component는 일반적인 선형 모델
    - $$y = w^Tx + b$$
    - y는 prediction(타겟), x는 features를 나타내는 벡터, w는 모델 파라미터 벡터, b는 편향
        - x는 희소 벡터(원핫 인코딩된 결과와 유사. 여기선 cross-product를 거친 후에 나오는 희소 벡터)
        - w는 이에 매핑되는 가중치 벡터
    - feature 셋은 인풋 feature의 원형과 변형된 feature가 포함됨
- 가장 중요한 transformations중 하나는 _cross-product transformation_
    - $$\phi _k (x) = \Pi_{i=1}^d x_{i}^{c_{ki}}\quad c_{ki} \in \{0, 1\}$$
    - $$c_{ki}$$는 boolean 변수
        - i번째 feature가 k번째 transformation $$\phi _k$$의 부분이면 1   아니면 0
- 이진 feature에서 교차곱(cross-product)변환은 만약 ("AND(gender=female, language=en)")가 (gender=female and language=en)을 모두 만족해야 1, 다른 경우엔 0이다
- 이는 이진 특성들 간의 상호작용을 포착하고 일반화된 선형 모델에 비선형성을 더한다
- _선형 연산으로는 잡히지 않는 특성을 기억(memorization)하기 위해 교차곱 변환으로 비선형성을 추가._

### Deep Component
- Deep Component : 기본적으로 feed-forward neural network
- 범주형 특성을 위해 원래의 인풋은 문자열 타입
    - "language=en"
- 이러한 희소하고 고차원의 범주형 특성들은 우선 저차원의 밀집한 실제 값의 벡터로 전환된다
    - 종종 임베딩 벡터로 나타내진다
    - 임베딩의 차원은 보통 _O(10)_ 에서 _O(100)_ 정도로 나타난다
- 임베딩 벡터는 랜덤하게 초기화되고, 값들은 모델 훈련 동안 최종 손실 함수를 최소화하게 훈련됨
- 이런 저차원 밀집 임베딩 벡터는 순방향으로 신경망의 은닉층으로 간다
- 각각의 은닉층은 다음과 같은 연산을 수행
    - $$a^{(l+1)} = f(W^{(l)}a^{(l)}+b^{(l)})$$
    - _l_ 은 레이어 번호, f는 활성화함수(ReLU)
    - $$a^{(l)}, b^{(l)}, W^{(l)}$$은 각각 l번째 은닉층의 활성값(그 은닉층을 거쳐 나온 값), 편향, 가중치